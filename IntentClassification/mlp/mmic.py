# -*- coding: utf-8 -*-
"""MMIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ObxGHWZCWoC1r_KotkhWONBHYV12IJwG

**Configuration options**

The configurable options are
*   Type of sentence embedding scheme which decides the input layer dimension
*   Number of visual features used
*   Hidden layer dimension
*   The number of identified output intent types
"""

# sentence embedding options are 'ConveRT','spacy'
SENTENCE_EMBEDDING_TYPE = 'ConveRT'    

VISUAL_FEATURES = 0                    # 3 visual features have been identified
                                          # distance from object to agent
                                          # distance from object to receptacle
                                          # distance from agent to receptacle


# the input layer dimension depends on the sentence embedding dimension and
# visual features count

if SENTENCE_EMBEDDING_TYPE == 'spacy':
  INPUT_LAYER_DIMENSION = 300 + VISUAL_FEATURES
else:
  INPUT_LAYER_DIMENSION = 1024 + VISUAL_FEATURES


# the hidden layer dimension of the multi-layer perceptron
HIDDEN_LAYER_DIMENSION = 20


# this corresponds to the output layer dimension
LABEL_CLASSES = 11

#
#         {
#          'GotoLocation': 0,
#          'PickupObject': 1,
#          'PutObject': 2,
#          'GotoLocation PickupObject': 3,
#          'GotoLocation PickupObject GotoLocation': 4,
#          'GotoLocation PickupObject GotoLocation PutObject': 5,
#          'PickupObject GotoLocation': 6,
#          'PickupObject GotoLocation PutObject': 7,
#          'GotoLocation PutObject': 8,
#          'GotoLocation PickupObject PutObject': 9,
#          'PickupObject PutObject': 10
#         }
#
#

TRAIN_MLP_FILE = '../data-train/train_mlp_full.txt'
TEST_MLP_FILE = '../data-train/dev_mlp_full.txt'

# This cell deals with accessing train and test data from google drive,
# as the runtime's uploaded data is cleared when the instance is restarted

import numpy as np
import pandas as pd

# Import the required dependencies

import tensorflow as tf
import tensorflow_text
import tensorflow_hub as tfhub

"""If using ConveRT for sentence embedding, download the ConveRT models from poly-ai.com"""

sess = None
if sess is not None:
    sess.close()

sess = tf.InteractiveSession(graph=tf.Graph())

module = tfhub.Module("http://models.poly-ai.com/convert/v1/model.tar.gz")

text_placeholder = tf.placeholder(dtype=tf.string, shape=[None])
encoding_tensor = module(text_placeholder)
encoding_dim = int(encoding_tensor.shape[1])
print(f"ConveRT encodes text to {encoding_dim}-dimensional vectors")

sess.run(tf.tables_initializer())
sess.run(tf.global_variables_initializer())

from torch.utils.data import Dataset


class MMICDataset(Dataset):

    def __init__(self, filename):
        # Store the contents of the file in a pandas dataframe
        self.df = pd.read_csv(filename, delimiter='\t')

    def __len__(self):
        return len(self.df)

    def get_embeddings(self, text):
      return sess.run(encoding_tensor, feed_dict={text_placeholder: text})[0]

    def __getitem__(self, index):

        # extract the language information
        desc = self.df.loc[index, 'desc']
        
        # extract the visual information
        dist_to_obj	= self.df.loc[index, 'dist_to_obj']
        dist_to_recep	= self.df.loc[index, 'dist_to_recep']
        dist_obj_to_recep = self.df.loc[index, 'dist_obj_to_recep']

        # extract the intent
        intent = self.df.loc[index, 'intent']

        if SENTENCE_EMBEDDING_TYPE == 'spacy':
          #  spacy representation
          doc = nlp(desc)
          sentence_rep = doc.vector
        elif SENTENCE_EMBEDDING_TYPE == 'ConveRT':
          # ConveRT representation
          sentence_rep = np.array(self.get_embeddings([desc]))

        if VISUAL_FEATURES > 0:
          visual_data = np.array([dist_to_obj, dist_to_recep, dist_obj_to_recep])
          # concatenate language and visual info
          data_input = np.append(sentence_rep, visual_data)
        else:
          data_input = sentence_rep

        # default all data input to float64 and convert to pytorch tensor
        doc_tensor = torch.from_numpy(data_input).float()
        label_tensor = torch.tensor(int(intent))

        return doc_tensor, label_tensor


# custom Dataset for train and test
train_set = MMICDataset(filename=TRAIN_MLP_FILE)
test_set = MMICDataset(filename=TEST_MLP_FILE)


from torch.utils.data import DataLoader

# Create train and test dataloaders
train_loader = DataLoader(train_set, batch_size=64, num_workers=0, pin_memory=True)
test_loader = DataLoader(test_set, batch_size=64, num_workers=0, pin_memory=True)


# utility method that calculates the accuracy
def accuracy(outputs, labels):
    # the output is a vector of dimension = LABEL_CLASSES
    # here we pick index of the dimension that has the highest value and
    # check if matches the label
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))


import torch
import torch.nn as nn
import torch.nn.functional as F


class IntentClassifier(nn.Module):

    def __init__(self):
        super(IntentClassifier, self).__init__()

        # input layer that accepts the language and visual features
        self.linear1 = nn.Linear(INPUT_LAYER_DIMENSION, HIDDEN_LAYER_DIMENSION)
        # output layer
        self.linear2 = nn.Linear(HIDDEN_LAYER_DIMENSION, LABEL_CLASSES)

    def forward(self, input_data):

        # feed the input
        out = self.linear1(input_data)
        # Apply RELU activation function
        out = F.relu(out)
        out = self.linear2(out)
        # out = func.softmax(out)
        return out

    # -------------------------------------------------------- #

    def training_step(self, batch):
        sentences, labels = batch 
        out = self(sentences)                  
        loss = F.cross_entropy(out, labels) 
        acc = accuracy(out, labels)            
        return {'loss': loss, 'acc': acc}
    
    def validation_step(self, batch):
        sentences, labels = batch 
        out = self(sentences)                  # Generate predictions
        loss = F.cross_entropy(out, labels)    # Calculate the loss
        acc = accuracy(out, labels)            # Calculate the accuracy
        return {'loss': loss, 'acc': acc}

    def train_epoch_end(self, outputs, epoch):
        batch_losses = [x['loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        result = {'loss': epoch_loss.item(), 'acc': epoch_acc.item()}
        self.display(epoch, result, 'train')
        return result
        
    def validation_epoch_end(self, outputs, epoch):
        batch_losses = [x['loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        result = {'loss': epoch_loss.item(), 'acc': epoch_acc.item()}
        self.display(epoch, result, 'test')
        return result

    def display(self, epoch, result, type):
        print(type)
        print("Epoch [{}], loss: {:.4f}, acc: {:.4f}".format(epoch, result['loss'], result['acc']))

model = IntentClassifier()

for t in model.parameters():
    print(t.shape)

for desc_commands, labels in train_loader:
    outputs = model(desc_commands)
    loss = F.cross_entropy(outputs, labels)
    print('Loss:', loss.item())
    break

def get_default_device():
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

device = get_default_device()
print(device)


def to_device(data, device):
    # Move tensors to the chosen device
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)


for desc_commands, label in train_loader:
    # input is taken in batches and the shape of desc_commands
    # should be BATCH_SIZE x INPUT_LAYER_DIMENSION 
    print(desc_commands.shape)
    desc_commands = to_device(desc_commands, device)
    print(desc_commands.device)
    break


class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        print('init')
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl:
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

# load data into GPU
train_loader = DeviceDataLoader(train_loader, device)
test_loader = DeviceDataLoader(test_loader, device)

for xb, yb in test_loader:
  print('xb.device:', xb.device)
  print('xb: ', xb)
  print('yb: ', yb)
  break

def evaluate(model, val_loader, epoch):
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs, epoch)

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    test_history = []
    train_history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):
        # Training Phase
        train_outputs = []
        for batch in train_loader:
            train_output = model.training_step(batch)
            train_outputs.append(train_output)
            train_output['loss'].backward()
            optimizer.step()
            optimizer.zero_grad()
        
        # Training phase
        train_result = model.train_epoch_end(train_outputs, epoch)
        train_history.append(train_result)
        # Validation phase
        result = evaluate(model, val_loader, epoch)
        test_history.append(result)
    return train_history, test_history

# run on GPU
model = IntentClassifier()
to_device(model, device)

#history = [evaluate(model, test_loader)]
train_history = []
test_history = []

train_history, test_history =  fit(5, 0.1, model, train_loader, test_loader)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

"""Plot the training accuracy and loss accuracy"""

accuracies = [x['acc'] for x in train_history]
plt.plot(accuracies, '-x')
plt.xlabel('epoch')
plt.ylabel('train accuracy')
plt.title('Accuracy vs Epochs');

losses = [x['loss'] for x in train_history]
plt.plot(losses, '-x')
plt.xlabel('epoch')
plt.ylabel('train loss')
plt.title('Loss vs Epochs')

losses = [x['loss'] for x in test_history]
plt.plot(losses, '-x')
plt.xlabel('epoch')
plt.ylabel('test loss')
plt.title('Loss vs Epochs')

train_hist, test_hist = fit(5, 0.1, model, train_loader, test_loader)
accuracies = [x['acc'] for x in train_hist]
plt.plot(accuracies, '-x')
plt.xlabel('epoch')
plt.ylabel('train accuracy')
plt.title('Accuracy vs Epochs');

accuracies = [x['acc'] for x in test_hist]
plt.plot(accuracies, '-x')
plt.xlabel('epoch')
plt.ylabel('test accuracy')
plt.title('Accuracy vs Epochs');

losses = [x['loss'] for x in train_hist]
plt.plot(losses, '-x')
plt.xlabel('epoch')
plt.ylabel('train loss')
plt.title('Loss vs Epochs');

losses = [x['loss'] for x in test_hist]
plt.plot(losses, '-x')
plt.xlabel('epoch')
plt.ylabel('test loss')
plt.title('Loss vs Epochs')

print(test_hist)
